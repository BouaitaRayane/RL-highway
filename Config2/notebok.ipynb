{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "import time\n",
    "import random\n",
    "import gymnasium as gym\n",
    "import highway_env\n",
    "import collections\n",
    "import os\n",
    "from configuration2 import config_dict\n",
    "\n",
    "# Vérifier si GPU est disponible et le configurer\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    print(f\"GPU disponible: {physical_devices}\")\n",
    "    # Configurer le GPU pour l'entraînement\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print(\"GPU configuré pour l'utilisation\")\n",
    "else:\n",
    "    print(\"Aucun GPU détecté, utilisation du CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch_size = min(batch_size, len(self.buffer))\n",
    "        samples = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*samples))\n",
    "        return states, actions, rewards.reshape(-1, 1), next_states, dones.reshape(-1, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor:\n",
    "    def __init__(self, state_dim, action_dim, action_bound):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.model = self._build_network()\n",
    "        self.optimizer = optimizers.Adam(learning_rate=0.001)  # Augmenté pour apprendre plus vite\n",
    "    \n",
    "    def _build_network(self):\n",
    "        inputs = layers.Input(shape=self.state_dim)\n",
    "        \n",
    "        # Aplatir l'entrée si elle est multidimensionnelle\n",
    "        x = layers.Flatten()(inputs)\n",
    "        \n",
    "        # Réseau plus petit et plus efficace\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "        x = layers.Dense(64, activation='relu')(x)\n",
    "        \n",
    "        outputs = layers.Dense(self.action_dim, activation='tanh')(x)\n",
    "        outputs = layers.Lambda(lambda x: x * self.action_bound)(outputs)\n",
    "        \n",
    "        model = models.Model(inputs, outputs)\n",
    "        return model\n",
    "    \n",
    "    def predict(self, state):\n",
    "        if len(state.shape) == len(self.state_dim):\n",
    "            state = np.expand_dims(state, axis=0)\n",
    "        return self.model(state)\n",
    "    \n",
    "    def train(self, states, critic_grads):\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = self.model(states)\n",
    "        actor_grads = tape.gradient(actions, self.model.trainable_variables, -critic_grads)\n",
    "        self.optimizer.apply_gradients(zip(actor_grads, self.model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.model = self._build_network()\n",
    "        self.optimizer = optimizers.Adam(learning_rate=0.002)\n",
    "    \n",
    "    def _build_network(self):\n",
    "        # Entrée d'état\n",
    "        state_input = layers.Input(shape=self.state_dim)\n",
    "        state_out = layers.Flatten()(state_input)\n",
    "        state_out = layers.Dense(128, activation='relu')(state_out)\n",
    "        \n",
    "        # Entrée d'action\n",
    "        action_input = layers.Input(shape=(self.action_dim,))\n",
    "        action_out = layers.Dense(64, activation='relu')(action_input)\n",
    "        \n",
    "        # Combiner les deux flux\n",
    "        concat = layers.Concatenate()([state_out, action_out])\n",
    "        x = layers.Dense(64, activation='relu')(concat)\n",
    "        outputs = layers.Dense(1)(x)\n",
    "        \n",
    "        model = models.Model([state_input, action_input], outputs)\n",
    "        return model\n",
    "    \n",
    "    def predict(self, state, action):\n",
    "        if len(state.shape) == len(self.state_dim):\n",
    "            state = np.expand_dims(state, axis=0)\n",
    "        if len(action.shape) == 1:\n",
    "            action = np.expand_dims(action, axis=0)\n",
    "        return self.model([state, action])\n",
    "    \n",
    "    def train(self, states, actions, target_q):\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.model([states, actions], training=True)\n",
    "            loss = tf.reduce_mean(tf.square(target_q - q_values))\n",
    "        \n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "        return loss\n",
    "    \n",
    "    def get_action_gradients(self, states, actions):\n",
    "        actions = tf.convert_to_tensor(actions)\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(actions)\n",
    "            q_values = self.model([states, actions])\n",
    "        return tape.gradient(q_values, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent:\n",
    "    def __init__(self, state_dim, action_dim, action_bound, buffer_capacity=50000, batch_size=128):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.buffer = ReplayBuffer(buffer_capacity)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = 0.99\n",
    "        self.tau = 0.005  # Augmenté pour une mise à jour plus rapide\n",
    "        self.train_interval = 4  # Entraîner plus fréquemment pour apprendre plus vite\n",
    "        self.steps_count = 0\n",
    "        \n",
    "        # Réseaux principaux\n",
    "        self.actor = Actor(state_dim, action_dim, action_bound)\n",
    "        self.critic = Critic(state_dim, action_dim)\n",
    "        \n",
    "        # Réseaux cibles\n",
    "        self.target_actor = Actor(state_dim, action_dim, action_bound)\n",
    "        self.target_critic = Critic(state_dim, action_dim)\n",
    "        \n",
    "        # Initialiser les poids des réseaux cibles\n",
    "        self.update_target_networks(tau=1.0)\n",
    "        \n",
    "        self.noise_std = 0.4  # Plus de bruit pour l'exploration initiale\n",
    "        self.noise_decay = 0.999  # Décroissance plus rapide du bruit\n",
    "        self.min_noise = 0.05\n",
    "    \n",
    "    def update_target_networks(self, tau=None):\n",
    "        tau = self.tau if tau is None else tau\n",
    "        \n",
    "        for source, target in zip(self.actor.model.variables, self.target_actor.model.variables):\n",
    "            target.assign((1 - tau) * target + tau * source)\n",
    "        \n",
    "        for source, target in zip(self.critic.model.variables, self.target_critic.model.variables):\n",
    "            target.assign((1 - tau) * target + tau * source)\n",
    "    \n",
    "    def get_action(self, state, add_noise=True):\n",
    "        state = np.array(state, dtype=np.float32)\n",
    "        if len(state.shape) == len(self.state_dim):\n",
    "            state = np.expand_dims(state, axis=0)\n",
    "        \n",
    "        action = self.actor.predict(state)[0]\n",
    "        \n",
    "        if add_noise:\n",
    "            noise = np.random.normal(0, self.noise_std, size=self.action_dim)\n",
    "            action = np.clip(action + noise, -self.action_bound, self.action_bound)\n",
    "            # Réduire le bruit au fil du temps\n",
    "            self.noise_std = max(self.min_noise, self.noise_std * self.noise_decay)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.buffer.add(state, action, reward, next_state, done)\n",
    "    \n",
    "    def train(self):\n",
    "        self.steps_count += 1\n",
    "        if self.steps_count % self.train_interval != 0 or len(self.buffer) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        # Échantillonner un batch d'expériences\n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Convertir en tenseurs TensorFlow\n",
    "        states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "        next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "        dones = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "        \n",
    "        # Entraîner le critique\n",
    "        target_actions = self.target_actor.predict(next_states)\n",
    "        target_q = self.target_critic.predict(next_states, target_actions)\n",
    "        target_q = rewards + self.gamma * target_q * (1 - dones)\n",
    "        \n",
    "        critic_loss = self.critic.train(states, actions, target_q)\n",
    "        \n",
    "        # Entraîner l'acteur\n",
    "        actions_pred = self.actor.predict(states)\n",
    "        critic_grads = self.critic.get_action_gradients(states, actions_pred)\n",
    "        self.actor.train(states, critic_grads)\n",
    "        \n",
    "        # Mettre à jour les réseaux cibles\n",
    "        self.update_target_networks()\n",
    "        \n",
    "        return critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ddpg(env, agent, episodes=1000, max_steps_per_episode=200, eval_interval=50):\n",
    "    \"\"\"Entraîne l'agent DDPG sans rendu pendant l'entraînement pour éviter les crashs\"\"\"\n",
    "    episode_rewards = []\n",
    "    episode_steps = []\n",
    "    critic_losses = []  # Pour suivre la loss du critique\n",
    "    best_reward = -float('inf')\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        truncated = False\n",
    "        step = 0\n",
    "        episode_losses = []  # Liste pour stocker les losses de cet épisode\n",
    "        \n",
    "        while not (done or truncated) and step < max_steps_per_episode:\n",
    "            # Sélectionner une action\n",
    "            action = agent.get_action(state)\n",
    "            \n",
    "            # Exécuter l'action\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            \n",
    "            # Stocker l'expérience\n",
    "            agent.remember(state, action, reward, next_state, done or truncated)\n",
    "            \n",
    "            # Entraîner l'agent et récupérer la loss\n",
    "            loss = agent.train()\n",
    "            if loss is not None:\n",
    "                episode_losses.append(float(loss))\n",
    "            \n",
    "            # Mise à jour de l'état et du compteur\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            step += 1\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_steps.append(step)\n",
    "        \n",
    "        # Calculer la loss moyenne de l'épisode\n",
    "        avg_loss = np.mean(episode_losses) if episode_losses else 0\n",
    "        critic_losses.append(avg_loss)\n",
    "        \n",
    "        # Afficher la progression tous les 10 épisodes\n",
    "        if episode % 10 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-10:])\n",
    "            print(f\"Épisode {episode}/{episodes}, Récompense: {episode_reward:.2f}, Moyenne des 10 dernières: {avg_reward:.2f}, Loss: {avg_loss:.4f}, Bruit: {agent.noise_std:.4f}, Étapes: {step}\")\n",
    "        \n",
    "        # Évaluation périodique sans rendu\n",
    "        if episode % eval_interval == 0:\n",
    "            eval_reward = evaluate_agent(env, agent, episodes=1, render=False)\n",
    "            print(f\"Évaluation à l'épisode {episode}: {eval_reward:.2f}\")\n",
    "            \n",
    "            # Sauvegarder le meilleur modèle\n",
    "            if eval_reward > best_reward:\n",
    "                best_reward = eval_reward\n",
    "                print(f\"Nouveau meilleur modèle trouvé! Récompense: {best_reward:.2f}\")\n",
    "    \n",
    "    return episode_rewards, episode_steps, critic_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, agent, episodes=5, render=True):\n",
    "    \"\"\"Évalue l'agent sur plusieurs épisodes\"\"\"\n",
    "    total_rewards = []\n",
    "    \n",
    "    # Configuration du rendu (uniquement si render=True)\n",
    "    fig = None\n",
    "    ax = None\n",
    "    img_plot = None\n",
    "    \n",
    "    if render:\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        plt.title(\"Évaluation de l'agent\")\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        truncated = False\n",
    "        step = 0\n",
    "        frames = []  # Pour collecter les images à afficher après\n",
    "        \n",
    "        while not (done or truncated) and step < 300:\n",
    "            # Choisir l'action sans bruit\n",
    "            action = agent.get_action(state, add_noise=False)\n",
    "            \n",
    "            # Exécuter l'action\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            \n",
    "            # Collecter les images pour affichage ultérieur si rendu demandé\n",
    "            if render:\n",
    "                frames.append(env.render())\n",
    "            \n",
    "            # Mettre à jour l'état et la récompense\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            step += 1\n",
    "        \n",
    "        # Afficher quelques images de l'épisode à la fin plutôt qu'en temps réel\n",
    "        if render and len(frames) > 0:\n",
    "            for i, frame in enumerate(frames[::10]):  # Afficher 1 image sur 10\n",
    "                plt.clf()\n",
    "                plt.imshow(frame)\n",
    "                plt.title(f\"Épisode {episode+1}, Step {i*10}, Reward: {episode_reward:.2f}\")\n",
    "                plt.pause(0.1)\n",
    "        \n",
    "        total_rewards.append(episode_reward)\n",
    "        print(f\"Épisode d'évaluation {episode+1}/{episodes}, Récompense: {episode_reward:.2f}, Étapes: {step}\")\n",
    "    \n",
    "    if fig is not None:\n",
    "        plt.close(fig)\n",
    "    \n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifier la configuration pour accélérer l'apprentissage et terminer quand l'agent sort de la route\n",
    "config_dict[\"simulation_frequency\"] = 30  # Simulation plus rapide\n",
    "config_dict[\"policy_frequency\"] = 10\n",
    "config_dict[\"duration\"] = 150  # Réduire pour des épisodes plus courts\n",
    "config_dict[\"offscreen_rendering\"] = True  # Désactiver le rendu pendant l'entraînement\n",
    "config_dict[\"render_mode\"] = \"rgb_array\"\n",
    "config_dict[\"offroad_terminal\"] = True  # S'assurer que l'épisode se termine quand l'agent sort de la route\n",
    "config_dict[\"terminal_on_out_of_road\"] = True  # Autre paramètre pour terminer l'épisode hors route\n",
    "config_dict[\"out_of_road_cost\"] = -15  # Pénalité plus forte si l'agent sort de la route\n",
    "\n",
    "# Créer et configurer l'environnement\n",
    "gym.register_envs(highway_env)\n",
    "env = gym.make(\"racetrack-v0\", render_mode=\"rgb_array\")\n",
    "env.unwrapped.configure(config_dict)\n",
    "\n",
    "# Observer les dimensions de l'état et de l'action\n",
    "state, _ = env.reset()\n",
    "print(f\"Forme de l'observation: {state.shape}\")\n",
    "print(f\"Espace d'action: {env.action_space}\")\n",
    "\n",
    "# Définir les dimensions d'état et d'action\n",
    "state_dim = state.shape\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_bound = 1.0\n",
    "\n",
    "# Créer l'agent DDPG avec des paramètres optimisés\n",
    "agent = DDPGAgent(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    action_bound=action_bound,\n",
    "    buffer_capacity=100000,\n",
    "    batch_size=256  # Plus grande batch size pour utiliser le GPU efficacement\n",
    ")\n",
    "\n",
    "print(f\"Agent créé avec state_dim={state_dim}, action_dim={action_dim}, action_bound={action_bound}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres d'entraînement\n",
    "episodes = 200  # Nombre total d'épisodes\n",
    "max_steps = 200  # Maximum d'étapes par épisode\n",
    "\n",
    "# Entraîner l'agent sans rendu pour éviter les crashs\n",
    "rewards, steps, critic_losses = train_ddpg(\n",
    "    env=env,\n",
    "    agent=agent,\n",
    "    episodes=episodes,\n",
    "    max_steps_per_episode=max_steps,\n",
    "    eval_interval=50  # Évaluer tous les 50 épisodes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Courbe de récompense\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(rewards)\n",
    "plt.title('Récompense par épisode')\n",
    "plt.xlabel('Épisode')\n",
    "plt.ylabel('Récompense')\n",
    "plt.grid(True)\n",
    "\n",
    "# Courbe du nombre d'étapes\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(steps)\n",
    "plt.title(\"Nombre d'étapes par épisode\")\n",
    "plt.xlabel('Épisode')\n",
    "plt.ylabel('Étapes')\n",
    "plt.grid(True)\n",
    "\n",
    "# Courbe des losses\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(critic_losses)\n",
    "plt.title(\"Loss du critique par épisode\")\n",
    "plt.xlabel('Épisode')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png')  # Sauvegarder l'image des courbes\n",
    "plt.show()\n",
    "\n",
    "# Calculer et afficher les moyennes glissantes\n",
    "window_size = 50\n",
    "rewards_smooth = [np.mean(rewards[max(0, i-window_size):i+1]) for i in range(len(rewards))]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(rewards, alpha=0.3, label='Récompenses brutes')\n",
    "plt.plot(rewards_smooth, label='Moyenne glissante')\n",
    "plt.title('Récompense par épisode (avec moyenne glissante)')\n",
    "plt.xlabel('Épisode')\n",
    "plt.ylabel('Récompense')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('rewards_smoothed.png')  # Sauvegarder l'image des récompenses lissées\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_agent(agent, num_episodes=3, max_steps=10000):\n",
    "    \"\"\"Démontre l'agent entraîné dans une fenêtre\"\"\"\n",
    "    # Créer un nouvel environnement pour la démonstration\n",
    "    demo_config = config_dict.copy()\n",
    "    demo_config[\"offscreen_rendering\"] = False  # Activer le rendu dans une fenêtre\n",
    "    demo_config[\"render_mode\"] = \"human\"  # Mode de rendu en temps réel\n",
    "    demo_config[\"screen_width\"] = 800  # Taille de la fenêtre plus grande\n",
    "    demo_config[\"screen_height\"] = 600\n",
    "    demo_config[\"simulation_frequency\"] = 15  # Ralentir un peu pour mieux voir\n",
    "    \n",
    "    # Créer l'environnement de démonstration\n",
    "    demo_env = gym.make(\"racetrack-v0\", render_mode=\"human\")\n",
    "    demo_env.unwrapped.configure(demo_config)\n",
    "    \n",
    "    print(\"\\nDémonstration de l'agent entraîné:\")\n",
    "    total_rewards = []\n",
    "    \n",
    "    try:\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = demo_env.reset()\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            truncated = False\n",
    "            step = 0\n",
    "            \n",
    "            while not (done or truncated) and step < max_steps:\n",
    "                # Choisir l'action sans exploration (sans bruit)\n",
    "                action = agent.get_action(state, add_noise=False)\n",
    "                \n",
    "                # Exécuter l'action\n",
    "                next_state, reward, done, truncated, _ = demo_env.step(action)\n",
    "                \n",
    "                # Mettre à jour l'état et la récompense\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                step += 1\n",
    "            \n",
    "            total_rewards.append(episode_reward)\n",
    "            print(f\"Épisode de démonstration {episode+1}/{num_episodes}, Récompense: {episode_reward:.2f}, Étapes: {step}\")\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Démonstration interrompue par l'utilisateur\")\n",
    "    \n",
    "    finally:\n",
    "        # Fermer l'environnement proprement\n",
    "        demo_env.close()\n",
    "        \n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    print(f\"Récompense moyenne de démonstration: {avg_reward:.2f}\")\n",
    "    \n",
    "    return avg_reward\n",
    "\n",
    "# Pour lancer la démonstration de l'agent après l'entraînement\n",
    "print(\"Appuyez sur Ctrl+C dans la fenêtre de sortie pour arrêter la démonstration.\")\n",
    "demonstrate_agent(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration pour l'évaluation finale avec sauvegarde d'images\n",
    "def evaluate_and_save_images(env, agent, episodes=3, max_steps=300):\n",
    "    \"\"\"Évalue l'agent et sauvegarde des images\"\"\"\n",
    "    import os\n",
    "    img_dir = \"evaluation_images\"\n",
    "    os.makedirs(img_dir, exist_ok=True)\n",
    "    \n",
    "    total_rewards = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        truncated = False\n",
    "        step = 0\n",
    "        frames = []\n",
    "        \n",
    "        while not (done or truncated) and step < max_steps:\n",
    "            # Choisir l'action sans bruit\n",
    "            action = agent.get_action(state, add_noise=False)\n",
    "            \n",
    "            # Exécuter l'action\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            \n",
    "            # Collecter l'image\n",
    "            frames.append(env.render())\n",
    "            \n",
    "            # Mettre à jour l'état et la récompense\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            step += 1\n",
    "        \n",
    "        # Sauvegarder des images clés de l'épisode\n",
    "        for i in range(0, len(frames), max(1, len(frames) // 10)):  # ~10 images par épisode\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.imshow(frames[i])\n",
    "            plt.title(f\"Épisode {episode+1}, Step {i}, Reward: {episode_reward:.2f}\")\n",
    "            plt.savefig(f\"{img_dir}/ep{episode+1}_step{i}.png\")\n",
    "            plt.close()\n",
    "        \n",
    "        # Sauvegarder l'image finale\n",
    "        if frames:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.imshow(frames[-1])\n",
    "            plt.title(f\"Épisode {episode+1} Final, Reward: {episode_reward:.2f}\")\n",
    "            plt.savefig(f\"{img_dir}/ep{episode+1}_final.png\")\n",
    "            plt.close()\n",
    "            \n",
    "        total_rewards.append(episode_reward)\n",
    "        print(f\"Épisode d'évaluation {episode+1}/{episodes}, Récompense: {episode_reward:.2f}, Étapes: {step}\")\n",
    "    \n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    print(f\"Récompense moyenne: {avg_reward:.2f}\")\n",
    "    print(f\"Images sauvegardées dans le dossier: {img_dir}\")\n",
    "    return avg_reward\n",
    "\n",
    "# Évaluer et sauvegarder des images\n",
    "print(\"\\nÉvaluation finale avec sauvegarde d'images:\")\n",
    "final_reward = evaluate_and_save_images(env, agent, episodes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_agent(agent, num_episodes=3, max_steps=300):\n",
    "    \"\"\"Démontre l'agent entraîné dans une fenêtre\"\"\"\n",
    "    # Créer un nouvel environnement pour la démonstration\n",
    "    demo_config = dict(config_dict)  # Copie du dictionnaire de configuration\n",
    "    demo_config[\"offscreen_rendering\"] = False\n",
    "    demo_config[\"render_mode\"] = \"human\"\n",
    "    demo_config[\"screen_width\"] = 800\n",
    "    demo_config[\"screen_height\"] = 600\n",
    "    demo_config[\"simulation_frequency\"] = 15  # Ralentir pour mieux voir\n",
    "    \n",
    "    # Créer l'environnement de démonstration\n",
    "    demo_env = gym.make(\"racetrack-v0\", render_mode=\"human\")\n",
    "    demo_env.unwrapped.configure(demo_config)\n",
    "    \n",
    "    print(\"\\nDémonstration de l'agent entraîné:\")\n",
    "    total_rewards = []\n",
    "    \n",
    "    try:\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = demo_env.reset()\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            truncated = False\n",
    "            step = 0\n",
    "            \n",
    "            while not (done or truncated) and step < max_steps:\n",
    "                # Choisir l'action sans exploration (sans bruit)\n",
    "                action = agent.get_action(state, add_noise=False)\n",
    "                \n",
    "                # Exécuter l'action\n",
    "                next_state, reward, done, truncated, _ = demo_env.step(action)\n",
    "                \n",
    "                # Mettre à jour l'état et la récompense\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                step += 1\n",
    "            \n",
    "            total_rewards.append(episode_reward)\n",
    "            print(f\"Épisode de démonstration {episode+1}/{num_episodes}, Récompense: {episode_reward:.2f}, Étapes: {step}\")\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Démonstration interrompue par l'utilisateur\")\n",
    "    \n",
    "    finally:\n",
    "        # Fermer l'environnement proprement\n",
    "        demo_env.close()\n",
    "        \n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    print(f\"Récompense moyenne de démonstration: {avg_reward:.2f}\")\n",
    "    \n",
    "    return avg_reward\n",
    "\n",
    "# Pour lancer la démonstration de l'agent après l'entraînement\n",
    "print(\"\\nDémonstration de l'agent dans une fenêtre:\")\n",
    "print(\"Appuyez sur Ctrl+C dans la fenêtre de sortie pour arrêter la démonstration.\")\n",
    "demonstrate_agent(agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
